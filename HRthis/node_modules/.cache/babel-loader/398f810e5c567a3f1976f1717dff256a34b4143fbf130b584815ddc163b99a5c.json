{"ast":null,"code":"/*\nIMPORTANT NOTICE: DO NOT REMOVE\n./src/api/chat-service.ts\nIf the user wants to use AI to generate text, answer questions, or analyze images you can use the functions defined in this file to communicate with the OpenAI, Anthropic, and Grok APIs.\n*/import{getAnthropicClient}from\"./anthropic\";import{getOpenAIClient}from\"./openai\";import{getGrokClient}from\"./grok\";/**\n * Get a text response from Anthropic\n * @param messages - The messages to send to the AI\n * @param options - The options for the request\n * @returns The response from the AI\n */export const getAnthropicTextResponse=async(messages,options)=>{try{var _response$usage,_response$usage2,_response$usage3,_response$usage4;const client=getAnthropicClient();const defaultModel=\"claude-3-5-sonnet-20240620\";const response=await client.messages.create({model:(options===null||options===void 0?void 0:options.model)||defaultModel,messages:messages.map(msg=>({role:msg.role===\"assistant\"?\"assistant\":\"user\",content:msg.content})),max_tokens:(options===null||options===void 0?void 0:options.maxTokens)||2048,temperature:(options===null||options===void 0?void 0:options.temperature)||0.7});// Handle content blocks from the response\nconst content=response.content.reduce((acc,block)=>{if(\"text\"in block){return acc+block.text;}return acc;},\"\");return{content,usage:{promptTokens:((_response$usage=response.usage)===null||_response$usage===void 0?void 0:_response$usage.input_tokens)||0,completionTokens:((_response$usage2=response.usage)===null||_response$usage2===void 0?void 0:_response$usage2.output_tokens)||0,totalTokens:(((_response$usage3=response.usage)===null||_response$usage3===void 0?void 0:_response$usage3.input_tokens)||0)+(((_response$usage4=response.usage)===null||_response$usage4===void 0?void 0:_response$usage4.output_tokens)||0)}};}catch(error){console.error(\"Anthropic API Error:\",error);throw error;}};/**\n * Get a simple chat response from Anthropic\n * @param prompt - The prompt to send to the AI\n * @returns The response from the AI\n */export const getAnthropicChatResponse=async prompt=>{return await getAnthropicTextResponse([{role:\"user\",content:prompt}]);};/**\n * Get a text response from OpenAI\n * @param messages - The messages to send to the AI\n * @param options - The options for the request\n * @returns The response from the AI\n */export const getOpenAITextResponse=async(messages,options)=>{try{var _options$temperature,_response$choices$,_response$choices$$me,_response$usage5,_response$usage6,_response$usage7;const client=getOpenAIClient();const defaultModel=\"gpt-4o\";//accepts images as well, use this for image analysis\nconst response=await client.chat.completions.create({model:(options===null||options===void 0?void 0:options.model)||defaultModel,messages:messages,temperature:(_options$temperature=options===null||options===void 0?void 0:options.temperature)!==null&&_options$temperature!==void 0?_options$temperature:0.7,max_tokens:(options===null||options===void 0?void 0:options.maxTokens)||2048});return{content:((_response$choices$=response.choices[0])===null||_response$choices$===void 0?void 0:(_response$choices$$me=_response$choices$.message)===null||_response$choices$$me===void 0?void 0:_response$choices$$me.content)||\"\",usage:{promptTokens:((_response$usage5=response.usage)===null||_response$usage5===void 0?void 0:_response$usage5.prompt_tokens)||0,completionTokens:((_response$usage6=response.usage)===null||_response$usage6===void 0?void 0:_response$usage6.completion_tokens)||0,totalTokens:((_response$usage7=response.usage)===null||_response$usage7===void 0?void 0:_response$usage7.total_tokens)||0}};}catch(error){console.error(\"OpenAI API Error:\",error);throw error;}};/**\n * Get a simple chat response from OpenAI\n * @param prompt - The prompt to send to the AI\n * @returns The response from the AI\n */export const getOpenAIChatResponse=async prompt=>{return await getOpenAITextResponse([{role:\"user\",content:prompt}]);};/**\n * Get a text response from Grok\n * @param messages - The messages to send to the AI\n * @param options - The options for the request\n * @returns The response from the AI\n */export const getGrokTextResponse=async(messages,options)=>{try{var _options$temperature2,_response$choices$2,_response$choices$2$m,_response$usage8,_response$usage9,_response$usage0;const client=getGrokClient();const defaultModel=\"grok-3-beta\";const response=await client.chat.completions.create({model:(options===null||options===void 0?void 0:options.model)||defaultModel,messages:messages,temperature:(_options$temperature2=options===null||options===void 0?void 0:options.temperature)!==null&&_options$temperature2!==void 0?_options$temperature2:0.7,max_tokens:(options===null||options===void 0?void 0:options.maxTokens)||2048});return{content:((_response$choices$2=response.choices[0])===null||_response$choices$2===void 0?void 0:(_response$choices$2$m=_response$choices$2.message)===null||_response$choices$2$m===void 0?void 0:_response$choices$2$m.content)||\"\",usage:{promptTokens:((_response$usage8=response.usage)===null||_response$usage8===void 0?void 0:_response$usage8.prompt_tokens)||0,completionTokens:((_response$usage9=response.usage)===null||_response$usage9===void 0?void 0:_response$usage9.completion_tokens)||0,totalTokens:((_response$usage0=response.usage)===null||_response$usage0===void 0?void 0:_response$usage0.total_tokens)||0}};}catch(error){console.error(\"Grok API Error:\",error);throw error;}};/**\n * Get a simple chat response from Grok\n * @param prompt - The prompt to send to the AI\n * @returns The response from the AI\n */export const getGrokChatResponse=async prompt=>{return await getGrokTextResponse([{role:\"user\",content:prompt}]);};","map":{"version":3,"names":["getAnthropicClient","getOpenAIClient","getGrokClient","getAnthropicTextResponse","messages","options","_response$usage","_response$usage2","_response$usage3","_response$usage4","client","defaultModel","response","create","model","map","msg","role","content","max_tokens","maxTokens","temperature","reduce","acc","block","text","usage","promptTokens","input_tokens","completionTokens","output_tokens","totalTokens","error","console","getAnthropicChatResponse","prompt","getOpenAITextResponse","_options$temperature","_response$choices$","_response$choices$$me","_response$usage5","_response$usage6","_response$usage7","chat","completions","choices","message","prompt_tokens","completion_tokens","total_tokens","getOpenAIChatResponse","getGrokTextResponse","_options$temperature2","_response$choices$2","_response$choices$2$m","_response$usage8","_response$usage9","_response$usage0","getGrokChatResponse"],"sources":["/Users/halteverbotsocialmacpro/Desktop/ars vivai/Worky Time/worky-time-web/src/api/chat-service.ts"],"sourcesContent":["/*\nIMPORTANT NOTICE: DO NOT REMOVE\n./src/api/chat-service.ts\nIf the user wants to use AI to generate text, answer questions, or analyze images you can use the functions defined in this file to communicate with the OpenAI, Anthropic, and Grok APIs.\n*/\nimport { AIMessage, AIRequestOptions, AIResponse } from \"../types/ai\";\nimport { getAnthropicClient } from \"./anthropic\";\nimport { getOpenAIClient } from \"./openai\";\nimport { getGrokClient } from \"./grok\";\n\n/**\n * Get a text response from Anthropic\n * @param messages - The messages to send to the AI\n * @param options - The options for the request\n * @returns The response from the AI\n */\nexport const getAnthropicTextResponse = async (\n  messages: AIMessage[],\n  options?: AIRequestOptions,\n): Promise<AIResponse> => {\n  try {\n    const client = getAnthropicClient();\n    const defaultModel = \"claude-3-5-sonnet-20240620\";\n\n    const response = await client.messages.create({\n      model: options?.model || defaultModel,\n      messages: messages.map((msg) => ({\n        role: msg.role === \"assistant\" ? \"assistant\" : \"user\",\n        content: msg.content,\n      })),\n      max_tokens: options?.maxTokens || 2048,\n      temperature: options?.temperature || 0.7,\n    });\n\n    // Handle content blocks from the response\n    const content = response.content.reduce((acc, block) => {\n      if (\"text\" in block) {\n        return acc + block.text;\n      }\n      return acc;\n    }, \"\");\n\n    return {\n      content,\n      usage: {\n        promptTokens: response.usage?.input_tokens || 0,\n        completionTokens: response.usage?.output_tokens || 0,\n        totalTokens: (response.usage?.input_tokens || 0) + (response.usage?.output_tokens || 0),\n      },\n    };\n  } catch (error) {\n    console.error(\"Anthropic API Error:\", error);\n    throw error;\n  }\n};\n\n/**\n * Get a simple chat response from Anthropic\n * @param prompt - The prompt to send to the AI\n * @returns The response from the AI\n */\nexport const getAnthropicChatResponse = async (prompt: string): Promise<AIResponse> => {\n  return await getAnthropicTextResponse([{ role: \"user\", content: prompt }]);\n};\n\n/**\n * Get a text response from OpenAI\n * @param messages - The messages to send to the AI\n * @param options - The options for the request\n * @returns The response from the AI\n */\nexport const getOpenAITextResponse = async (messages: AIMessage[], options?: AIRequestOptions): Promise<AIResponse> => {\n  try {\n    const client = getOpenAIClient();\n    const defaultModel = \"gpt-4o\"; //accepts images as well, use this for image analysis\n\n    const response = await client.chat.completions.create({\n      model: options?.model || defaultModel,\n      messages: messages,\n      temperature: options?.temperature ?? 0.7,\n      max_tokens: options?.maxTokens || 2048,\n    });\n\n    return {\n      content: response.choices[0]?.message?.content || \"\",\n      usage: {\n        promptTokens: response.usage?.prompt_tokens || 0,\n        completionTokens: response.usage?.completion_tokens || 0,\n        totalTokens: response.usage?.total_tokens || 0,\n      },\n    };\n  } catch (error) {\n    console.error(\"OpenAI API Error:\", error);\n    throw error;\n  }\n};\n\n/**\n * Get a simple chat response from OpenAI\n * @param prompt - The prompt to send to the AI\n * @returns The response from the AI\n */\nexport const getOpenAIChatResponse = async (prompt: string): Promise<AIResponse> => {\n  return await getOpenAITextResponse([{ role: \"user\", content: prompt }]);\n};\n\n/**\n * Get a text response from Grok\n * @param messages - The messages to send to the AI\n * @param options - The options for the request\n * @returns The response from the AI\n */\nexport const getGrokTextResponse = async (messages: AIMessage[], options?: AIRequestOptions): Promise<AIResponse> => {\n  try {\n    const client = getGrokClient();\n    const defaultModel = \"grok-3-beta\";\n\n    const response = await client.chat.completions.create({\n      model: options?.model || defaultModel,\n      messages: messages,\n      temperature: options?.temperature ?? 0.7,\n      max_tokens: options?.maxTokens || 2048,\n    });\n\n    return {\n      content: response.choices[0]?.message?.content || \"\",\n      usage: {\n        promptTokens: response.usage?.prompt_tokens || 0,\n        completionTokens: response.usage?.completion_tokens || 0,\n        totalTokens: response.usage?.total_tokens || 0,\n      },\n    };\n  } catch (error) {\n    console.error(\"Grok API Error:\", error);\n    throw error;\n  }\n};\n\n/**\n * Get a simple chat response from Grok\n * @param prompt - The prompt to send to the AI\n * @returns The response from the AI\n */\nexport const getGrokChatResponse = async (prompt: string): Promise<AIResponse> => {\n  return await getGrokTextResponse([{ role: \"user\", content: prompt }]);\n};"],"mappings":"AAAA;AACA;AACA;AACA;AACA,EAEA,OAASA,kBAAkB,KAAQ,aAAa,CAChD,OAASC,eAAe,KAAQ,UAAU,CAC1C,OAASC,aAAa,KAAQ,QAAQ,CAEtC;AACA;AACA;AACA;AACA;AACA,GACA,MAAO,MAAM,CAAAC,wBAAwB,CAAG,KAAAA,CACtCC,QAAqB,CACrBC,OAA0B,GACF,CACxB,GAAI,KAAAC,eAAA,CAAAC,gBAAA,CAAAC,gBAAA,CAAAC,gBAAA,CACF,KAAM,CAAAC,MAAM,CAAGV,kBAAkB,CAAC,CAAC,CACnC,KAAM,CAAAW,YAAY,CAAG,4BAA4B,CAEjD,KAAM,CAAAC,QAAQ,CAAG,KAAM,CAAAF,MAAM,CAACN,QAAQ,CAACS,MAAM,CAAC,CAC5CC,KAAK,CAAE,CAAAT,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAES,KAAK,GAAIH,YAAY,CACrCP,QAAQ,CAAEA,QAAQ,CAACW,GAAG,CAAEC,GAAG,GAAM,CAC/BC,IAAI,CAAED,GAAG,CAACC,IAAI,GAAK,WAAW,CAAG,WAAW,CAAG,MAAM,CACrDC,OAAO,CAAEF,GAAG,CAACE,OACf,CAAC,CAAC,CAAC,CACHC,UAAU,CAAE,CAAAd,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAEe,SAAS,GAAI,IAAI,CACtCC,WAAW,CAAE,CAAAhB,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAEgB,WAAW,GAAI,GACvC,CAAC,CAAC,CAEF;AACA,KAAM,CAAAH,OAAO,CAAGN,QAAQ,CAACM,OAAO,CAACI,MAAM,CAAC,CAACC,GAAG,CAAEC,KAAK,GAAK,CACtD,GAAI,MAAM,EAAI,CAAAA,KAAK,CAAE,CACnB,MAAO,CAAAD,GAAG,CAAGC,KAAK,CAACC,IAAI,CACzB,CACA,MAAO,CAAAF,GAAG,CACZ,CAAC,CAAE,EAAE,CAAC,CAEN,MAAO,CACLL,OAAO,CACPQ,KAAK,CAAE,CACLC,YAAY,CAAE,EAAArB,eAAA,CAAAM,QAAQ,CAACc,KAAK,UAAApB,eAAA,iBAAdA,eAAA,CAAgBsB,YAAY,GAAI,CAAC,CAC/CC,gBAAgB,CAAE,EAAAtB,gBAAA,CAAAK,QAAQ,CAACc,KAAK,UAAAnB,gBAAA,iBAAdA,gBAAA,CAAgBuB,aAAa,GAAI,CAAC,CACpDC,WAAW,CAAE,CAAC,EAAAvB,gBAAA,CAAAI,QAAQ,CAACc,KAAK,UAAAlB,gBAAA,iBAAdA,gBAAA,CAAgBoB,YAAY,GAAI,CAAC,GAAK,EAAAnB,gBAAA,CAAAG,QAAQ,CAACc,KAAK,UAAAjB,gBAAA,iBAAdA,gBAAA,CAAgBqB,aAAa,GAAI,CAAC,CACxF,CACF,CAAC,CACH,CAAE,MAAOE,KAAK,CAAE,CACdC,OAAO,CAACD,KAAK,CAAC,sBAAsB,CAAEA,KAAK,CAAC,CAC5C,KAAM,CAAAA,KAAK,CACb,CACF,CAAC,CAED;AACA;AACA;AACA;AACA,GACA,MAAO,MAAM,CAAAE,wBAAwB,CAAG,KAAO,CAAAC,MAAc,EAA0B,CACrF,MAAO,MAAM,CAAAhC,wBAAwB,CAAC,CAAC,CAAEc,IAAI,CAAE,MAAM,CAAEC,OAAO,CAAEiB,MAAO,CAAC,CAAC,CAAC,CAC5E,CAAC,CAED;AACA;AACA;AACA;AACA;AACA,GACA,MAAO,MAAM,CAAAC,qBAAqB,CAAG,KAAAA,CAAOhC,QAAqB,CAAEC,OAA0B,GAA0B,CACrH,GAAI,KAAAgC,oBAAA,CAAAC,kBAAA,CAAAC,qBAAA,CAAAC,gBAAA,CAAAC,gBAAA,CAAAC,gBAAA,CACF,KAAM,CAAAhC,MAAM,CAAGT,eAAe,CAAC,CAAC,CAChC,KAAM,CAAAU,YAAY,CAAG,QAAQ,CAAE;AAE/B,KAAM,CAAAC,QAAQ,CAAG,KAAM,CAAAF,MAAM,CAACiC,IAAI,CAACC,WAAW,CAAC/B,MAAM,CAAC,CACpDC,KAAK,CAAE,CAAAT,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAES,KAAK,GAAIH,YAAY,CACrCP,QAAQ,CAAEA,QAAQ,CAClBiB,WAAW,EAAAgB,oBAAA,CAAEhC,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAEgB,WAAW,UAAAgB,oBAAA,UAAAA,oBAAA,CAAI,GAAG,CACxClB,UAAU,CAAE,CAAAd,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAEe,SAAS,GAAI,IACpC,CAAC,CAAC,CAEF,MAAO,CACLF,OAAO,CAAE,EAAAoB,kBAAA,CAAA1B,QAAQ,CAACiC,OAAO,CAAC,CAAC,CAAC,UAAAP,kBAAA,kBAAAC,qBAAA,CAAnBD,kBAAA,CAAqBQ,OAAO,UAAAP,qBAAA,iBAA5BA,qBAAA,CAA8BrB,OAAO,GAAI,EAAE,CACpDQ,KAAK,CAAE,CACLC,YAAY,CAAE,EAAAa,gBAAA,CAAA5B,QAAQ,CAACc,KAAK,UAAAc,gBAAA,iBAAdA,gBAAA,CAAgBO,aAAa,GAAI,CAAC,CAChDlB,gBAAgB,CAAE,EAAAY,gBAAA,CAAA7B,QAAQ,CAACc,KAAK,UAAAe,gBAAA,iBAAdA,gBAAA,CAAgBO,iBAAiB,GAAI,CAAC,CACxDjB,WAAW,CAAE,EAAAW,gBAAA,CAAA9B,QAAQ,CAACc,KAAK,UAAAgB,gBAAA,iBAAdA,gBAAA,CAAgBO,YAAY,GAAI,CAC/C,CACF,CAAC,CACH,CAAE,MAAOjB,KAAK,CAAE,CACdC,OAAO,CAACD,KAAK,CAAC,mBAAmB,CAAEA,KAAK,CAAC,CACzC,KAAM,CAAAA,KAAK,CACb,CACF,CAAC,CAED;AACA;AACA;AACA;AACA,GACA,MAAO,MAAM,CAAAkB,qBAAqB,CAAG,KAAO,CAAAf,MAAc,EAA0B,CAClF,MAAO,MAAM,CAAAC,qBAAqB,CAAC,CAAC,CAAEnB,IAAI,CAAE,MAAM,CAAEC,OAAO,CAAEiB,MAAO,CAAC,CAAC,CAAC,CACzE,CAAC,CAED;AACA;AACA;AACA;AACA;AACA,GACA,MAAO,MAAM,CAAAgB,mBAAmB,CAAG,KAAAA,CAAO/C,QAAqB,CAAEC,OAA0B,GAA0B,CACnH,GAAI,KAAA+C,qBAAA,CAAAC,mBAAA,CAAAC,qBAAA,CAAAC,gBAAA,CAAAC,gBAAA,CAAAC,gBAAA,CACF,KAAM,CAAA/C,MAAM,CAAGR,aAAa,CAAC,CAAC,CAC9B,KAAM,CAAAS,YAAY,CAAG,aAAa,CAElC,KAAM,CAAAC,QAAQ,CAAG,KAAM,CAAAF,MAAM,CAACiC,IAAI,CAACC,WAAW,CAAC/B,MAAM,CAAC,CACpDC,KAAK,CAAE,CAAAT,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAES,KAAK,GAAIH,YAAY,CACrCP,QAAQ,CAAEA,QAAQ,CAClBiB,WAAW,EAAA+B,qBAAA,CAAE/C,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAEgB,WAAW,UAAA+B,qBAAA,UAAAA,qBAAA,CAAI,GAAG,CACxCjC,UAAU,CAAE,CAAAd,OAAO,SAAPA,OAAO,iBAAPA,OAAO,CAAEe,SAAS,GAAI,IACpC,CAAC,CAAC,CAEF,MAAO,CACLF,OAAO,CAAE,EAAAmC,mBAAA,CAAAzC,QAAQ,CAACiC,OAAO,CAAC,CAAC,CAAC,UAAAQ,mBAAA,kBAAAC,qBAAA,CAAnBD,mBAAA,CAAqBP,OAAO,UAAAQ,qBAAA,iBAA5BA,qBAAA,CAA8BpC,OAAO,GAAI,EAAE,CACpDQ,KAAK,CAAE,CACLC,YAAY,CAAE,EAAA4B,gBAAA,CAAA3C,QAAQ,CAACc,KAAK,UAAA6B,gBAAA,iBAAdA,gBAAA,CAAgBR,aAAa,GAAI,CAAC,CAChDlB,gBAAgB,CAAE,EAAA2B,gBAAA,CAAA5C,QAAQ,CAACc,KAAK,UAAA8B,gBAAA,iBAAdA,gBAAA,CAAgBR,iBAAiB,GAAI,CAAC,CACxDjB,WAAW,CAAE,EAAA0B,gBAAA,CAAA7C,QAAQ,CAACc,KAAK,UAAA+B,gBAAA,iBAAdA,gBAAA,CAAgBR,YAAY,GAAI,CAC/C,CACF,CAAC,CACH,CAAE,MAAOjB,KAAK,CAAE,CACdC,OAAO,CAACD,KAAK,CAAC,iBAAiB,CAAEA,KAAK,CAAC,CACvC,KAAM,CAAAA,KAAK,CACb,CACF,CAAC,CAED;AACA;AACA;AACA;AACA,GACA,MAAO,MAAM,CAAA0B,mBAAmB,CAAG,KAAO,CAAAvB,MAAc,EAA0B,CAChF,MAAO,MAAM,CAAAgB,mBAAmB,CAAC,CAAC,CAAElC,IAAI,CAAE,MAAM,CAAEC,OAAO,CAAEiB,MAAO,CAAC,CAAC,CAAC,CACvE,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}